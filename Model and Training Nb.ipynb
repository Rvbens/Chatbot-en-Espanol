{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pickle, random, itertools, os\n",
    "import numpy as np\n",
    "\n",
    "from pre_processing import Voc\n",
    "from settings import *\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data and batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_dir + '/voc.pkl',  'rb') as f:\n",
    "    voc   = pickle.load(f)\n",
    "    \n",
    "with open(save_dir + '/pairs.pkl','rb') as f:\n",
    "    pairs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeroPadding(l, fillvalue=PAD_token):\n",
    "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
    "\n",
    "def binaryMatrix(l, value=PAD_token):\n",
    "    m = []\n",
    "    for i, seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if token == PAD_token:\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "    return m\n",
    "\n",
    "# Returns padded input sequence tensor and lengths\n",
    "def inputVar(indexes_batch):\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, lengths\n",
    "\n",
    "# Returns padded target sequence tensor, padding mask, and max target length\n",
    "def outputVar(indexes_batch):\n",
    "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    mask = binaryMatrix(padList)\n",
    "    mask = torch.ByteTensor(mask)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, mask, max_target_len\n",
    "\n",
    "# Returns all items for a given batch of pairs\n",
    "def batch2TrainData(pair_batch):\n",
    "    pair_batch.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    input_batch, output_batch = [], []\n",
    "    for pair in pair_batch:\n",
    "        input_batch.append(pair[0])\n",
    "        output_batch.append(pair[1])\n",
    "    inp, lengths = inputVar(input_batch)\n",
    "    output, mask, max_target_len = outputVar(output_batch)\n",
    "    return inp, lengths, output, mask, max_target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variable:\n",
      " tensor([[   37,   886,    36,  2368,    83,    37],\n",
      "        [   59,    92,    37,   175,  1085,   263],\n",
      "        [   17,  2108,    64,   802,  7024,    44],\n",
      "        [11665,    13,  2245,    41,  1664,  3713],\n",
      "        [ 3943,   898,   461,  1129,    85,    43],\n",
      "        [  239,    69,    46,  5002,     2,     2],\n",
      "        [   25,  3649,   618,  2434,     0,     0],\n",
      "        [  416, 11519,    43,    21,     0,     0],\n",
      "        [   13,    21,     2,     2,     0,     0],\n",
      "        [   17,     2,     0,     0,     0,     0],\n",
      "        [ 1117,     0,     0,     0,     0,     0],\n",
      "        [   43,     0,     0,     0,     0,     0],\n",
      "        [    2,     0,     0,     0,     0,     0]])\n",
      "lengths: tensor([13, 10,  9,  9,  6,  6])\n",
      "target_variable:\n",
      " tensor([[ 8003,    37,    36,    31,    83, 20482],\n",
      "        [   21,  1283,    37,  3675,   191, 14623],\n",
      "        [    2,    43,    64,    21,   229,   102],\n",
      "        [    0,     2,   407,     2,  2368, 35249],\n",
      "        [    0,     0,    97,     0,    55, 35162],\n",
      "        [    0,     0,    85,     0,    39,    21],\n",
      "        [    0,     0,     2,     0,  3012,     2],\n",
      "        [    0,     0,     0,     0,  3011,     0],\n",
      "        [    0,     0,     0,     0,    85,     0],\n",
      "        [    0,     0,     0,     0,     2,     0]])\n",
      "mask:\n",
      " tensor([[1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1],\n",
      "        [0, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 1, 0, 1, 1],\n",
      "        [0, 0, 1, 0, 1, 1],\n",
      "        [0, 0, 1, 0, 1, 1],\n",
      "        [0, 0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 1, 0]], dtype=torch.uint8)\n",
      "max_target_len: 10\n"
     ]
    }
   ],
   "source": [
    "# Example for validation\n",
    "small_batch_size = 6\n",
    "pairs_list = [random.choice(pairs) for _ in range(small_batch_size)]\n",
    "batches = batch2TrainData(pairs_list)\n",
    "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
    "\n",
    "print(\"input_variable:\\n\", input_variable)\n",
    "print(\"lengths:\", lengths)\n",
    "print(\"target_variable:\\n\", target_variable)\n",
    "print(\"mask:\\n\", mask)\n",
    "print(\"max_target_len:\", max_target_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "\n",
    "        # Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size'\n",
    "        #   because our input size is a word embedding with number of features == hidden_size\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
    "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
    "\n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        # Convert word indexes to embeddings\n",
    "        embedded = self.embedding(input_seq)\n",
    "        # Pack padded batch of sequences for RNN module\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        # Forward pass through GRU\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        # Unpack padding\n",
    "        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        # Sum bidirectional GRU outputs\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
    "        # Return output and final hidden state\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Luong attention layer\n",
    "class Attn(torch.nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        if self.method not in ['dot', 'general', 'concat']:\n",
    "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
    "        self.hidden_size = hidden_size\n",
    "        if self.method == 'general':\n",
    "            self.attn = torch.nn.Linear(self.hidden_size, hidden_size)\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = torch.nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = torch.nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "\n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        return torch.sum(hidden * encoder_output, dim=2)\n",
    "\n",
    "    def general_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(encoder_output)\n",
    "        return torch.sum(hidden * energy, dim=2)\n",
    "\n",
    "    def concat_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
    "        return torch.sum(self.v * energy, dim=2)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # Calculate the attention weights (energies) based on the given method\n",
    "        if self.method == 'general':\n",
    "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'concat':\n",
    "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'dot':\n",
    "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
    "\n",
    "        # Transpose max_length and batch_size dimensions\n",
    "        attn_energies = attn_energies.t()\n",
    "\n",
    "        # Return the softmax normalized probability scores (with added dimension)\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "\n",
    "        # Keep for reference\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Define layers\n",
    "        self.embedding = embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step (word) at a time\n",
    "        # Get embedding of current input word\n",
    "        embedded = self.embedding(input_step)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        # Forward through unidirectional GRU\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "        # Calculate attention weights from the current GRU output\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "        # Concatenate weighted context vector and GRU output using Luong eq. 5\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "        # Predict next word using Luong eq. 6\n",
    "        output = self.out(concat_output)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        # Return output and final hidden state\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskNLLLoss(inp, target, mask):\n",
    "    nTotal = mask.sum()\n",
    "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
    "    loss = crossEntropy.masked_select(mask).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, nTotal.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding,\n",
    "          encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=MAX_LENGTH):\n",
    "\n",
    "    # Zero gradients\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # Set device options\n",
    "    input_variable = input_variable.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    target_variable = target_variable.to(device)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    # Initialize variables\n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "\n",
    "    # Forward pass through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "\n",
    "    # Create initial decoder input (start with SOS tokens for each sentence)\n",
    "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
    "    decoder_input = decoder_input.to(device)\n",
    "\n",
    "    # Set initial decoder hidden state to the encoder's final hidden state\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "\n",
    "    # Determine if we are using teacher forcing this iteration\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    # Forward batch of sequences through decoder one time step at a time\n",
    "    if use_teacher_forcing:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # Teacher forcing: next input is current target\n",
    "            decoder_input = target_variable[t].view(1, -1)\n",
    "            # Calculate and accumulate loss\n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "    else:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            # No teacher forcing: next input is decoder's own current output\n",
    "            _, topi = decoder_output.topk(1)\n",
    "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            # Calculate and accumulate loss\n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "\n",
    "    # Perform backpropatation\n",
    "    loss.backward()\n",
    "\n",
    "    # Clip gradients: gradients are modified in place\n",
    "    _ = torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    _ = torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "\n",
    "    # Adjust model weights\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return sum(print_losses) / n_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, \n",
    "               encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, \n",
    "               clip, corpus_name, loadFilename, total_iter):\n",
    "\n",
    "    # Load batches for each iteration\n",
    "    #TODO: generator form to save memory of training_batches\n",
    "    training_batches = [batch2TrainData([random.choice(pairs) for _ in range(batch_size)])\n",
    "                      for _ in range(n_iteration)]\n",
    "\n",
    "    # Initializations\n",
    "#     print('Initializing ...')\n",
    "    start_iteration = 0\n",
    "    print_loss = 0\n",
    "    if loadFilename:\n",
    "        start_iteration = checkpoint['iteration']\n",
    "\n",
    "    # Training loop\n",
    "#     print(\"Training...\")\n",
    "    for iteration in range(1,n_iteration+1):\n",
    "        training_batch = training_batches[iteration-1]\n",
    "        # Extract fields from batch\n",
    "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
    "\n",
    "        # Run a training iteration with batch\n",
    "        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n",
    "                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
    "        print_loss += loss\n",
    "\n",
    "        # Print progress\n",
    "        if iteration % print_every == 0:\n",
    "            print_loss_avg = print_loss / print_every\n",
    "            assert np.isnan(print_loss_avg) == False\n",
    "            print(f\"Iteration: {iteration}; Percent complete: {iteration / n_iteration * 100:.1f}%; \"\n",
    "                  f\"Average loss: {print_loss_avg:.4f}\")\n",
    "            print_loss = 0\n",
    "\n",
    "        # Save checkpoint\n",
    "        if (iteration % save_every == 0):\n",
    "            directory = os.path.join(save_dir, \"checkpoints\")\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            total_iter += iteration\n",
    "            torch.save({\n",
    "                'iteration': start_iteration+iteration,\n",
    "                'en': encoder.state_dict(),\n",
    "                'de': decoder.state_dict(),\n",
    "                'en_opt': encoder_optimizer.state_dict(),\n",
    "                'de_opt': decoder_optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                'voc_dict': voc.__dict__,\n",
    "                'embedding': embedding.state_dict(),\n",
    "                \"total_iter\": total_iter,#for plotting\n",
    "                'loss': print_loss_avg   #for plotting\n",
    "            }, os.path.join(directory, f'{start_iteration+iteration}_checkpoint.tar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building encoder and decoder ...\n",
      "Nº of params:  67007301\n",
      "Models built and ready to go!\n"
     ]
    }
   ],
   "source": [
    "# Configure models\n",
    "attn_model = 'dot'\n",
    "#attn_model = 'general'\n",
    "#attn_model = 'concat'\n",
    "hidden_size = 500 #tamaño del embedding y del hidden\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 2\n",
    "dropout = 0.1\n",
    "batch_size = 256\n",
    "\n",
    "# Set checkpoint to load from; set to None if starting from scratch\n",
    "train_from_zero = False\n",
    "load_last_iter  = True\n",
    "chkpt_dir = os.path.join(save_dir, \"checkpoints\")\n",
    "\n",
    "if train_from_zero:\n",
    "    loadFilename = None\n",
    "    total_iter = 0\n",
    "else:\n",
    "    if load_last_iter: #load last save\n",
    "        chkpt = os.listdir(chkpt_dir)[-1]\n",
    "        loadFilename = os.path.join(chkpt_dir,chkpt)\n",
    "    else:    \n",
    "        checkpoint_iter = 20_000 #load especific iter\n",
    "        loadFilename = os.path.join(chkpt_dir, f'{checkpoint_iter}_checkpoint.tar')\n",
    "\n",
    "print('Building encoder and decoder ...')\n",
    "# Initialize word embeddings\n",
    "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "\n",
    "# Initialize encoder & decoder models\n",
    "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n",
    "print(\"Nº of params: \", count_parameters(encoder) + count_parameters(decoder))\n",
    "\n",
    "# Use appropriate device\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "# Load model if a loadFilename is provided\n",
    "if loadFilename:\n",
    "    # If loading on same machine the model was trained on\n",
    "    checkpoint = torch.load(loadFilename)\n",
    "    # If loading a model trained on GPU to CPU\n",
    "    #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))+\n",
    "    encoder.load_state_dict(checkpoint['en'])\n",
    "    decoder.load_state_dict(checkpoint['de'])\n",
    "    embedding.load_state_dict(checkpoint['embedding'])\n",
    "    voc.__dict__ = checkpoint['voc_dict']\n",
    "    total_iter = checkpoint[\"total_iter\"]\n",
    "    \n",
    "print('Models built and ready to go!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training/optimization\n",
    "clip = 15.0\n",
    "teacher_forcing_ratio = 0.99\n",
    "learning_rate = 4e-5\n",
    "decoder_learning_ratio = 5.0\n",
    "n_iteration = 10_000\n",
    "print_every = 2_000\n",
    "save_every = 10_000\n",
    "wd = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building optimizers ...\n"
     ]
    }
   ],
   "source": [
    "# Initialize optimizers\n",
    "print('Building optimizers ...')\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate,weight_decay=wd)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio,weight_decay=wd)\n",
    "if loadFilename:\n",
    "    encoder_optimizer.load_state_dict(checkpoint['en_opt'])\n",
    "    decoder_optimizer.load_state_dict(checkpoint['de_opt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2000; Percent complete: 20.0%; Average loss: 3.5752\n",
      "Iteration: 4000; Percent complete: 40.0%; Average loss: 3.5560\n",
      "Iteration: 6000; Percent complete: 60.0%; Average loss: 3.5512\n",
      "Iteration: 8000; Percent complete: 80.0%; Average loss: 3.5583\n",
      "Iteration: 10000; Percent complete: 100.0%; Average loss: 3.5562\n",
      "Iteration: 2000; Percent complete: 20.0%; Average loss: 3.5498\n",
      "Iteration: 4000; Percent complete: 40.0%; Average loss: 3.5546\n",
      "Iteration: 6000; Percent complete: 60.0%; Average loss: 3.5591\n",
      "Iteration: 8000; Percent complete: 80.0%; Average loss: 3.5427\n",
      "Iteration: 10000; Percent complete: 100.0%; Average loss: 3.5418\n",
      "Iteration: 2000; Percent complete: 20.0%; Average loss: 3.5514\n",
      "Iteration: 4000; Percent complete: 40.0%; Average loss: 3.5512\n",
      "Iteration: 6000; Percent complete: 60.0%; Average loss: 3.5466\n",
      "Iteration: 8000; Percent complete: 80.0%; Average loss: 3.5261\n",
      "Iteration: 10000; Percent complete: 100.0%; Average loss: 3.5424\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-a004425796da>\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename, total_iter)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Run a training iteration with batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n\u001b[0;32m---> 26\u001b[0;31m                      decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mprint_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-82f8f89f4df7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip, max_length)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_target_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             decoder_output, decoder_hidden = decoder(\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             )\n\u001b[1;32m     38\u001b[0m             \u001b[0;31m# Teacher forcing: next input is current target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-6ea60ef8f190>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_step, last_hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Note: we run this one step (word) at a time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# Get embedding of current input word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_dropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Forward through unidirectional GRU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m         return F.embedding(\n\u001b[1;32m    117\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1452\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1454\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Ensure dropout layers are in train mode\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "# Run training iterations\n",
    "for _ in range(6):\n",
    "#     print(\"Starting Training!\")\n",
    "    trainIters(voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "               embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n",
    "               print_every, save_every, clip, corpus_name, loadFilename,total_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Eval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from pre_processing import process_punct, indexesFromSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def custom_capitalize(s):\n",
    "    for i, c in enumerate(s):\n",
    "        if c.isalpha():\n",
    "            break\n",
    "    return s[:i] + s[i:].capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def reformatString(l):\n",
    "    s = l.strip().lower()\n",
    "    s = re.sub(r\"<guion_inic>\",r\"\", s)\n",
    "    s = re.sub(r\"\\s+([.!?])\", r\"\\1\", s)\n",
    "    s = re.sub(r\"([¡¿])\\s+\", r\"\\1\", s)\n",
    "#     s = re.sub(r\"\\s+\", r\" \", s)\n",
    "    return custom_capitalize(s).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n",
    "    ### Format input sentence as a batch\n",
    "    # words -> indexes\n",
    "    indexes_batch = [indexesFromSentence(sentence, voc)]\n",
    "    # Create lengths tensor\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    # Transpose dimensions of batch to match models' expectations\n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
    "    # Use appropriate device\n",
    "    input_batch = input_batch.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    # Decode sentence with searcher\n",
    "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
    "    # indexes -> words\n",
    "    decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
    "    return decoded_words\n",
    "\n",
    "\n",
    "def evaluateInput(encoder, decoder, searcher, voc):\n",
    "    input_sentence = ''\n",
    "    while(1):\n",
    "        try:\n",
    "            # Get input sentence\n",
    "            input_sentence = input('> ')\n",
    "            # Check if it is quit case\n",
    "            if input_sentence == 'q' or input_sentence == 'quit': break\n",
    "            # Normalize sentence\n",
    "            input_sentence = \"-\"+input_sentence #para que siga el formato de guiones de una conversacion\n",
    "            input_sentence = process_punct(input_sentence.encode())\n",
    "#             print(input_sentence)\n",
    "            # Evaluate sentence\n",
    "            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "            # Format and print response sentence\n",
    "            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
    "            raw_ans = ' '.join(output_words)\n",
    "            ans = reformatString(raw_ans)\n",
    "            print('Bot:',ans)\n",
    "\n",
    "        except KeyError:\n",
    "            print(\"Error: Encountered unknown word.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class GreedySearchDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(GreedySearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_seq, input_length, max_length):\n",
    "        # Forward input through encoder model\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "        # Prepare encoder's final hidden layer to be first hidden input to the decoder\n",
    "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "        # Initialize decoder input with SOS_token\n",
    "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
    "        # Initialize tensors to append decoded words to\n",
    "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
    "        all_scores = torch.zeros([0], device=device)\n",
    "        # Iteratively decode one word token at a time\n",
    "        for _ in range(max_length):\n",
    "            # Forward pass through decoder\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            # Obtain most likely word token and its softmax score\n",
    "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
    "            # Record token and score\n",
    "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
    "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
    "            # Prepare current token to be next decoder input (add a dimension)\n",
    "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "        # Return collections of word tokens and scores\n",
    "        return all_tokens, all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> hola\n",
      "Bot: \" I \". \" \"\n",
      "> que te pasa\n",
      "Bot: En los medios , los resultados en los medios.\n",
      "> hola\n",
      "Bot: \" I \". \" \"\n",
      "> hola.\n",
      "Bot: \" I \"..\n",
      "> Hola\n",
      "Bot: \" I \". \" \"\n",
      "> - Hola\n",
      "Bot: \" I \". \" \" \" \"\n",
      "> Estás loca\n",
      "Bot: \" I \".\n",
      "> que cojones\n",
      "Bot: En serio.\n",
      "> Dí algo con sentido\n",
      "Bot: Soy juez.\n",
      "> No lo eres\n",
      "Bot: ¿Qué les digo?.\n",
      "> ¿A quién?\n",
      "Bot: En parte.\n",
      "> Di algo con más palabras\n",
      "Bot: \" I \". \" i \"\n",
      "> i\n",
      "Bot: I feel together.\n",
      "> \"i\"\n",
      "Bot: \" I \". \" \" \" \"\n",
      "> i\n",
      "Bot: I feel together.\n",
      "> feel\n",
      "Bot: Si , señor , no me lo puedo servir.\n",
      "> togheter\n",
      "Bot: \" I ' c. \"\n",
      "> together\n",
      "Bot: \" I ' c. \"\n",
      "> english\n",
      "Bot: I feel feel feel feel together.\n",
      "> loco\n",
      "Bot: \" \" Sólo \" \" \" \" \" \" \" \" \" \" \"\n",
      "> locura\n",
      "Bot: \" I ' m \". \" \" \" \" \" \" \"\n",
      "> que pasa\n",
      "Bot: En serio.\n",
      "> un cuento\n",
      "Bot: ¿Cómo?\n",
      "> cuenta\n",
      "Bot: ¿Cómo?\n",
      "> uno\n",
      "Bot: ¿Cómo me vamos?\n",
      "> dos\n",
      "Bot: ¿Cómo?\n",
      "> tres\n",
      "Bot: ¿Cómo?\n",
      "> un león\n",
      "Bot: ¿Cómo me vamos?\n",
      "> piedra\n",
      "Bot: \" I ' m \".\n",
      "> papekl\n",
      "Bot: \" I \". \" i \"\n",
      "> papel\n",
      "Bot: \" I \". \" a pesar \". \" \" \" \"\n",
      "> piso\n",
      "Bot: \" I ' m \".\n",
      "> el piso\n",
      "Bot: \" I ' m \".\n",
      "> cuentame un cuento\n",
      "Bot: ¿Cómo?\n",
      "> tengo un regalo para t\n",
      "Bot: ( Music ) } ) } que te lo dije ) } ) } ) } ) } ) }\n",
      "> tengo un regalo para ti\n",
      "Bot: ( Music ) } ) } que te lo dije ) } ) } que te ha cortado ) }\n",
      "> cantame una cancion\n",
      "Bot: Enemigos.\n",
      "> donde\n",
      "Bot: Soy feliz , soy un gran libro.\n",
      "> te gusta leer?\n",
      "Bot: \" I ' you ' gran you? \"?????\n",
      "> I\n",
      "Bot: I feel together.\n",
      "> i\n",
      "Bot: I feel together.\n",
      "> fuck\n",
      "Bot: I feel i feel together. i i i i i i i i i i i i i\n",
      "> eres mexicana\n",
      "Bot: En parte del libro.\n",
      "> del que libro\n",
      "Bot: \" I ' m , también me dijo que no me dijo nada \".\n",
      "> has aprendido algo de inglés\n",
      "Bot: Soy presidente y los cargos en el cuadrante alfa.\n",
      "> como tiene que ser\n",
      "Bot: Todo en los ángeles.\n",
      "> y cada uno en su casa\n",
      "Bot: En los medios tienen que tratar en los necesitados.\n",
      "> porque lo necesitan\n",
      "Bot: Libro y } que me gusta????\n",
      "> te gustan los libros\n",
      "Bot: ¿Cómo te atreves?\n",
      "> lo siento\n",
      "Bot: ¿Cómo?\n",
      "> ¿me perdonas?\n",
      "Bot: En parte del libro.\n",
      "> ¿de qué libro?\n",
      "Bot: ¿Cómo?\n",
      "> comiendo\n",
      "Bot: ¿Cómo?\n",
      "> hijo de \n",
      "Bot: Soy feliz.\n",
      "> hijo de puta\n",
      "Bot: I ' c. ' c???????\n",
      "> lo siento\n",
      "Bot: ¿Cómo?\n",
      "> me estoy cabreando\n",
      "Bot: ¿Cómo?\n",
      "> titled\n",
      "Bot: ¿Cómo?\n",
      "> uff\n",
      "Bot: \" I \".\n",
      "> tu\n",
      "Bot: \" I ' c \".\n",
      "> y yo\n",
      "Bot: \" En serio. \"\n",
      "> quita las comillas\n",
      "Bot: I ' m , i ' m , i ' you ' c????\n",
      "> hombre\n",
      "Bot: ¿Qué me dice?.\n",
      "> tiburón\n",
      "Bot: I feel feel feel together.\n",
      "> máquina\n",
      "Bot: I feel feel together.\n",
      "> ¿es eso una canción?\n",
      "Bot: En parte del libro.\n",
      "> del guión\n",
      "Bot: I feel feel together.\n",
      "> sabes qué\n",
      "Bot: En parte , soy yo!\n",
      "> tu eres el libro\n",
      "Bot: \" I ' m , también me dijo que no me dijo. \"\n",
      "> sabias palabras\n",
      "Bot: I ' m , i ' i ' i ' i ' i ' i ' i ' i '\n",
      "> vaya crack\n",
      "Bot: \" I \". \" i \". \" i \" \" \"\n",
      "> una vez \n",
      "Bot: \" I \". \". \".\n",
      "> joder\n",
      "Bot: Soy un gran libro.\n",
      "> si señora\n",
      "Bot: \" I ' m , en serio. \"\n",
      "> no lo dudo\n",
      "Bot: \" I \"..\n",
      "> am\n",
      "Bot: ¿Cómo?\n",
      "> i ' m\n",
      "Bot: ¿Cómo?\n",
      "> eres\n",
      "Bot: ¿Cómo?\n",
      "> un\n",
      "Bot: \" I \"\n",
      "> roboy\n",
      "Bot: ¿Me equivoco?..\n",
      "> robot\n",
      "Bot: ¿Cómo?.\n",
      "> automanta\n",
      "Bot: ¿Cómo?\n",
      "> automata\n",
      "Bot: ¿Cómo?.\n",
      "> como un coche pero mejor\n",
      "Bot: \" En serio \".\n",
      "> sii\n",
      "Bot: I feel i feel together. i i i i i i i i i i i\n",
      "> jajaj\n",
      "Bot: Por noticias , por mí ,..\n",
      "> por todos mis compañeros\n",
      "Bot: Libro..\n",
      "> el libro sagrado\n",
      "Bot: \" I ' m también. \"\n",
      "> eres bilinggue\n",
      "Bot: Sr. presidente , sr. cain , sr. cain , sr. i. i.\n",
      "> cain\n",
      "Bot: I feel feel\n",
      "> dios\n",
      "Bot: I ' m. i ' m i ' m. i ' m. i '\n",
      "> y la santa paloma\n",
      "Bot: \" I \". \" no? \"\n",
      "> no\n",
      "Bot: ¿Qué?\n",
      "> aprende más y hablamos luego\n",
      "Bot: \" I ' m y me mando en el cuadrante alfa \"\n",
      "> exacto\n",
      "Bot: ¿Cómo?\n",
      "> adios\n",
      "Bot: I ' m. i ' m i ' m. i ' m. i '\n",
      "> bye\n",
      "Bot: Si me equivoco , me gusta.\n",
      "> a mi también\n",
      "Bot: I feel feel?\n",
      "> i feel good\n",
      "Bot: \" I \".\n",
      "> how you feel0\n",
      "Bot: \" I \".\n",
      "> \" I \".\n",
      "Bot: \" I \". \" no me puedo quedar en una hora. \"\n",
      "> vale vale me voy\n",
      "Bot: \" ¡Silencio \"!..\n",
      "> no te pongas tonta\n",
      "Bot: ¿Qué les parece eso?\n",
      "> mal te voy a desenchufar\n",
      "Bot: \" I \".. \".\n",
      "> no te pones tan chulita ahora\n",
      "Bot: \" I ' you ' er , soy papa. \"\n",
      "> buen intento\n",
      "Bot: ¿Cómo?\n",
      "> hasta luego\n",
      "Bot: \" I \".\n",
      ">  \n",
      "Bot: \" I \".\n",
      ">  \n",
      "Bot: \" I \".\n",
      ">  \n",
      "Bot: \" I \".\n",
      ">  \n",
      "Bot: \" I \".\n",
      ">  \n",
      "Bot: \" I \".\n",
      ">  \n",
      "Bot: \" I \".\n",
      ">  \n",
      "Bot: \" I \".\n",
      ">  \n",
      "Bot: \" I \".\n",
      ">  \n",
      "Bot: \" I \".\n",
      ">  \n",
      "Bot: \" I \".\n",
      ">  \n",
      "Bot: \" I \".\n",
      ">  \n",
      "Bot: \" I \".\n",
      ">  \n",
      "Bot: \" I \".\n",
      ">  \n",
      "Bot: \" I \".\n",
      ">  \n",
      "Bot: \" I \".\n",
      ">  \n",
      "Bot: \" I \".\n",
      ">  \n",
      "Bot: \" I \".\n",
      "> a\n",
      "Bot: !\n",
      "> s\n",
      "Bot: En serio.\n",
      "> d\n",
      "Bot: Soy un gran libro.\n",
      "> f\n",
      "Bot: \" I \".\n",
      "> g\n",
      "Bot: \" I \".\n",
      "> h\n",
      "Bot: \" I \".\n",
      "> f\n",
      "Bot: \" I \".\n",
      "> f\n",
      "Bot: \" I \".\n",
      "> f\n",
      "Bot: \" I \".\n",
      "> f\n",
      "Bot: \" I \".\n",
      "> f\n",
      "Bot: \" I \".\n",
      "> f\n",
      "Bot: \" I \".\n",
      "> \n",
      "Bot: \" I \".\n",
      "> \n",
      "Bot: \" I \".\n",
      "> \n",
      "Bot: \" I \".\n",
      "> \n",
      "Bot: \" I \".\n",
      "> f\n",
      "Bot: \" I \".\n",
      "> f\n",
      "Bot: \" I \".\n",
      "> f\n",
      "Bot: \" I \".\n",
      "> f\n",
      "Bot: \" I \".\n",
      "> gf\n",
      "Bot: \" I \".\n",
      "> g\n",
      "Bot: \" I \".\n",
      "> s\n",
      "Bot: En serio.\n",
      "> gs\n",
      "Bot: \" I \".\n",
      "> d\n",
      "Bot: Soy un gran libro.\n",
      "> fs\n",
      "Bot: En serio.\n",
      "> dg\n",
      "Bot: Soy un gran testigo.\n",
      "> j\n",
      "Bot: En serio..\n",
      "> q\n"
     ]
    }
   ],
   "source": [
    "# Set dropout layers to eval mode\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Initialize search module\n",
    "searcher = GreedySearchDecoder(encoder, decoder)\n",
    "\n",
    "# Begin chatting (uncomment and run the following line to begin)\n",
    "evaluateInput(encoder, decoder, searcher, voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
